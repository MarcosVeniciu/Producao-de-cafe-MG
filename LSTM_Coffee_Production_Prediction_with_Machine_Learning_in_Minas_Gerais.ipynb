{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7FXKQua4bjZ-",
        "7tREOe8Mb9C1"
      ],
      "authorship_tag": "ABX9TyMpkcgV1F5dUb+UUoEWC2tY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcosVeniciu/Producao-de-cafe-MG/blob/main/LSTM_Coffee_Production_Prediction_with_Machine_Learning_in_Minas_Gerais.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpfp55EzbMPS"
      },
      "outputs": [],
      "source": [
        "# requeriment\n",
        "%%capture\n",
        "%pip install neuralforecast statsforecast\n",
        "%pip install optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funções de suporte"
      ],
      "metadata": {
        "id": "7FXKQua4bjZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score, root_mean_squared_error\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "from neuralforecast import NeuralForecast\n",
        "from IPython.display import clear_output\n",
        "from neuralforecast.models import LSTM\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import optuna\n",
        "import math\n",
        "import os"
      ],
      "metadata": {
        "id": "i0RDBFRodewu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(\n",
        "    train_df,\n",
        "    test_df,\n",
        "    exog_list,\n",
        "    h: int,\n",
        "    input_size: int,\n",
        "    output_dir: str = './model',\n",
        "    steps: int = 500,\n",
        "    default_params: dict = None,\n",
        "    n_trials: int = 20,\n",
        "    val_df=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Treina um modelo LSTM usando os parâmetros em default_params.\n",
        "    - Se default_params for um dict, executa o treinamento com esses parâmetros.\n",
        "    - Se default_params for None, exibe um aviso e não prossegue com o treinamento.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Verifica se os parâmetros foram fornecidos\n",
        "    if default_params is None:\n",
        "        print(\"Você deve informar os parametros a serem usados no treinamento!\")\n",
        "        return None\n",
        "\n",
        "    # Branch sem Optuna: usa os parâmetros fornecidos\n",
        "    model = LSTM(\n",
        "        h=h,\n",
        "        input_size=input_size,\n",
        "        **default_params\n",
        "    )\n",
        "    nf = NeuralForecast(models=[model], freq=\"YE\")\n",
        "    nf.fit(df=train_df)\n",
        "    nf.save(path=output_dir, overwrite=True, save_dataset=False)\n",
        "\n",
        "    # Empacota os artefatos em um ZIP\n",
        "    zip_path = Path(output_dir) / \"model_bundle.zip\"\n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "        for root, _, files in os.walk(output_dir):\n",
        "            for f in files:\n",
        "                if f != zip_path.name:\n",
        "                    zf.write(os.path.join(root, f), arcname=f)\n",
        "    print(f\"\\nModel artifacts zipped to {zip_path}\")"
      ],
      "metadata": {
        "id": "V5c-Ty2TbnBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rolling_evaluation(model, full_df: pd.DataFrame, test_df: pd.DataFrame, context_size: int, horizon: int, inteiro=True) -> pd.DataFrame:\n",
        "  # --- Preparação inicial ---\n",
        "  # Garantir que as datas estejam em datetime e ordenar os dataframes\n",
        "  full_df = full_df.copy()\n",
        "  test_df = test_df.copy()\n",
        "  full_df['ds'] = pd.to_datetime(full_df['ds'])\n",
        "  test_df['ds'] = pd.to_datetime(test_df['ds'])\n",
        "\n",
        "  # Todos os valores exclusivos de datas no período de teste, ordenados\n",
        "  test_dates = sorted(test_df['ds'].unique())\n",
        "  # Determina quantas janelas de previsão iremos executar\n",
        "  n_dates = len(test_dates)\n",
        "  n_loops = math.ceil(n_dates / horizon)\n",
        "\n",
        "  all_forecasts = []\n",
        "\n",
        "  # --- Loop de previsão por janelas de tamanho `horizon` ---\n",
        "  for j in range(n_loops):\n",
        "      # Identifica o bloco de datas que compõem o horizonte atual\n",
        "      date_chunk = test_dates[j * horizon : (j + 1) * horizon]\n",
        "      start_date = date_chunk[0]  # Data de início dessa janela de previsão\n",
        "\n",
        "      print(f\"\\nExecutando window {j+1}/{n_loops}: datas {date_chunk[0].date()} a {date_chunk[-1].date()}\")\n",
        "      val_df = test_df[test_df['ds'].isin(date_chunk)] # possui os dados de teste para a janela atual\n",
        "      history_df = full_df[full_df['ds'] < start_date] # possui os dados historicos para a janela atual\n",
        "      futr_df = val_df.drop(columns=[\"y\"]).copy()\n",
        "\n",
        "      # Realiza a predição para a janela atual\n",
        "      forecasts_df = model.predict(\n",
        "        df=history_df,         # Dados históricos (para contexto)\n",
        "        futr_df=futr_df      # Valores futuros das variáveis exógenas\n",
        "      )\n",
        "      # Combinar previsões com valores reais do teste\n",
        "      evaluation_df = forecasts_df.merge(\n",
        "          val_df[[\"unique_id\", \"ds\", \"y\"]],\n",
        "          on=[\"unique_id\", \"ds\"]\n",
        "      )\n",
        "      evaluation_df['y'] = np.expm1(evaluation_df['y'])\n",
        "      evaluation_df['LSTM'] = np.expm1(evaluation_df['LSTM'])\n",
        "\n",
        "      if inteiro:\n",
        "        evaluation_df['LSTM'] = evaluation_df['LSTM'].round().astype(int)\n",
        "        evaluation_df['y'] = evaluation_df['y'].round().astype(int)\n",
        "\n",
        "      all_forecasts.append(evaluation_df)\n",
        "\n",
        "  # --- Concatena todas as previsões obtidas ---\n",
        "  forecasts_df = pd.concat(all_forecasts, ignore_index=True)\n",
        "  forecasts_df = calcular_diferenca_percentual(forecasts_df)\n",
        "\n",
        "  return forecasts_df"
      ],
      "metadata": {
        "id": "vOuheeIUjQgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_steps_options():\n",
        "  # Intervalos com granularidade fina no início\n",
        "  step_1 = list(range(500, 1001, 100))\n",
        "  step_2 = list(range(1100, 2001, 200))\n",
        "  step_3 = list(range(2200, 5001, 400))\n",
        "\n",
        "  steps_options = step_1 + step_2 + step_3\n",
        "  return sorted(steps_options)"
      ],
      "metadata": {
        "id": "qJABAATFzm1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## WMAPE (Weighted MAPE)\n",
        "def wmape(actual, predicted):\n",
        "  return np.sum(np.abs(predicted - actual)) / np.sum(np.abs(actual))"
      ],
      "metadata": {
        "id": "6M4jIJoHhHR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calcular_diferenca_percentual(df):\n",
        "    \"\"\"\n",
        "    Calcula a diferença percentual entre valores preditos (LSTM) e reais (y)\n",
        "    e retorna um novo DataFrame com a coluna adicional 'diferença_%'\n",
        "\n",
        "    Fórmula: ((LSTM - y) / y) * 100\n",
        "\n",
        "    Interpretação:\n",
        "    - Valor negativo: previsão subestimada (LSTM < y)\n",
        "    - Valor positivo: previsão superestimada (LSTM > y)\n",
        "\n",
        "    Parâmetros:\n",
        "    df (DataFrame): DataFrame com colunas 'y' e 'LSTM'\n",
        "\n",
        "    Retorna:\n",
        "    DataFrame: Novo DataFrame com a coluna 'diferença_%' adicionada\n",
        "    \"\"\"\n",
        "    # Cria uma cópia do DataFrame para não modificar o original\n",
        "    df_novo = df.copy()\n",
        "\n",
        "    # Calcula a diferença percentual com sinal correto\n",
        "    df_novo['diferença_%'] = round(((df_novo['LSTM'] - df_novo['y']) / df_novo['y']) * 100, 2)\n",
        "\n",
        "    return df_novo"
      ],
      "metadata": {
        "id": "rVBwOZJSjM42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparar dataset"
      ],
      "metadata": {
        "id": "9N5MPG0Eb1_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('/content/dataset_v6.csv')\n",
        "print(f\"Total de dados: {len(dataset)}\")"
      ],
      "metadata": {
        "id": "UxBTGb6fb3rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = [\n",
        "    'target',\n",
        "    'Municipio',\n",
        "    'Ano',\n",
        "    'latitude',\n",
        "    'longitude',\n",
        "    'altitude',\n",
        "    'precipitacao (mm)',\n",
        "    'temperatura minima (ºC)',\n",
        "    'temperatura maxima (ºC)',\n",
        "]\n",
        "cat_features = [\n",
        "  'Mesorregião'\n",
        "]\n",
        "dataset = dataset[num_features + cat_features].copy()"
      ],
      "metadata": {
        "id": "_SWezrzodjf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Log-transform em Área colhida, Quantidade (em mil toneladas) e Valor da produção."
      ],
      "metadata": {
        "id": "6wnkORhdA6JZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_cols = [\n",
        "  'Área colhida (Hectares)',\n",
        "  'target',\n",
        "  'Valor da produção (Mil Reais)'\n",
        "]\n",
        "\n",
        "# Verifica quais colunas de log_cols estão presentes no dataset\n",
        "existing_log_cols = [col for col in log_cols if col in dataset.columns]\n",
        "\n",
        "# Mostra as colunas que serão transformadas\n",
        "print(\"Colunas encontradas para aplicar log1p:\", existing_log_cols)\n",
        "\n",
        "# Aplica log1p apenas nas colunas presentes\n",
        "dataset[existing_log_cols] = dataset[existing_log_cols].apply(lambda x: np.log1p(x))"
      ],
      "metadata": {
        "id": "Ac-qLbE_A6JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Renomeando as colunas para o formato esperado pelo NeuralForecast\n",
        "dataset = dataset.rename(columns={\n",
        "  \"Municipio\": \"unique_id\",\n",
        "  \"Ano\": \"ds\",\n",
        "  \"target\": \"y\"\n",
        "})\n",
        "dataset = dataset.sort_values(by=[\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
        "# Converte o ano inteiro para uma string no formato 'YYYY-12-31' (último dia do ano)\n",
        "dataset['ds'] = pd.to_datetime(dataset['ds'].astype(str) + '-12-31')"
      ],
      "metadata": {
        "id": "AuRPRmNpUHwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializa o OneHotEncoder com as configurações desejadas\n",
        "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
        "\n",
        "# Aplica o encoder na coluna \"Mesorregião\" e converte para DataFrame\n",
        "encoded_data = encoder.fit_transform(dataset[[\"Mesorregião\"]])\n",
        "encoded_df = pd.DataFrame(\n",
        "    encoded_data,\n",
        "    columns=encoder.get_feature_names_out([\"Mesorregião\"])\n",
        ")\n",
        "\n",
        "# Concatena o resultado com o dataset original e remove a coluna original\n",
        "dataset = pd.concat([dataset, encoded_df], axis=1).drop(columns=[\"Mesorregião\"])"
      ],
      "metadata": {
        "id": "s-__DowYalJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exog_list = [col for col in dataset.columns.tolist() if col not in [\"ds\", \"y\", \"unique_id\"]]\n",
        "print(f\"Variaveis Exogenas:\")\n",
        "for col in exog_list:\n",
        "  print(f\" - {col}\")"
      ],
      "metadata": {
        "id": "ObvNIuwHjtoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ano = 2023 # ano de inicio dos testes.\n",
        "\n",
        "train_ds = dataset[dataset['ds'].dt.year < ano]\n",
        "test_ds = dataset[dataset['ds'].dt.year >= ano]\n",
        "\n",
        "print(f\"Total de dados de treino: {len(train_ds)}\")\n",
        "print(f\"Total de dados de teste: {len(test_ds)}\")"
      ],
      "metadata": {
        "id": "MOa5Cwmfd709"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treinamento"
      ],
      "metadata": {
        "id": "pVgNtXjOb4Q9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normal"
      ],
      "metadata": {
        "id": "7tREOe8Mb9C1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h = 1\n",
        "input_size = 4\n",
        "max_steps = 800\n",
        "default_params = {\n",
        "  'batch_size': 32,\n",
        "  'scaler_type': 'revin',\n",
        "  'encoder_dropout': 0.2,\n",
        "  'encoder_n_layers': 8,\n",
        "  'encoder_hidden_size': 154,\n",
        "  'decoder_layers': 1,\n",
        "  'decoder_hidden_size': 117,\n",
        "  'futr_exog_list': exog_list,\n",
        "  'learning_rate': 0.002388703885848156,\n",
        "  'max_steps': max_steps,\n",
        "  'loss': HuberLoss(delta=1.0),\n",
        "  'lr_scheduler': CyclicLR,\n",
        "  'lr_scheduler_kwargs': {\n",
        "      'base_lr': 1e-4,\n",
        "      'max_lr': 1e-2,\n",
        "      'step_size_up': int(max_steps * 0.45),\n",
        "      'mode': 'triangular',\n",
        "      'cycle_momentum': False\n",
        "  }\n",
        "}\n",
        "\n",
        "model_output = f\"model_{max_steps}\"\n",
        "train_model(\n",
        "    train_df=train_ds,\n",
        "    test_df=test_ds,\n",
        "    exog_list=exog_list,\n",
        "    h=h,\n",
        "    input_size=input_size,\n",
        "    default_params=default_params,\n",
        "    output_dir=f'./{model_output}'\n",
        ")"
      ],
      "metadata": {
        "id": "T8BDk6_7b5AB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning"
      ],
      "metadata": {
        "id": "61jln_mYb_UH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h= 1\n",
        "input_size = 4\n",
        "steps_options = generate_steps_options()"
      ],
      "metadata": {
        "id": "krt-mUIVy4GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local = \"previsoes_2023\"\n",
        "\n",
        "best_score_global = float(\"inf\")\n",
        "\n",
        "def objective(trial):\n",
        "    global best_score_global\n",
        "\n",
        "    # hiperparâmetros\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
        "    encoder_n_layers = trial.suggest_int(\"encoder_n_layers\", 3, 8)\n",
        "    encoder_hidden_size = trial.suggest_int(\"encoder_hidden_size\", 64, 256)\n",
        "    decoder_layers = trial.suggest_int(\"decoder_layers\", 1, 4)\n",
        "    decoder_hidden_size = trial.suggest_int(\"decoder_hidden_size\", 64, 256)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
        "    steps = trial.suggest_categorical(\"steps\", steps_options)\n",
        "\n",
        "    # Instanciar o modelo LSTM com os parâmetros sugeridos\n",
        "    model = LSTM(\n",
        "        h=h,\n",
        "        input_size=input_size,\n",
        "        batch_size=batch_size,\n",
        "        scaler_type='revin',\n",
        "        encoder_dropout=0.3,\n",
        "        encoder_n_layers=encoder_n_layers,\n",
        "        encoder_hidden_size=encoder_hidden_size,\n",
        "        decoder_layers=decoder_layers,\n",
        "        decoder_hidden_size=decoder_hidden_size,\n",
        "        futr_exog_list=exog_list,\n",
        "        learning_rate=learning_rate,\n",
        "        max_steps=steps,\n",
        "        lr_scheduler=CyclicLR,\n",
        "        lr_scheduler_kwargs={\n",
        "            \"base_lr\": 1e-4,\n",
        "            \"max_lr\": 1e-2,\n",
        "            \"step_size_up\": int(steps * 0.45),\n",
        "            \"mode\": \"triangular\",\n",
        "            \"cycle_momentum\": False\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Treinar o modelo usando NeuralForecast\n",
        "    nf = NeuralForecast(models=[model], freq=\"YE\")  # \"MS\" = início de mês, YE = Anual\n",
        "    nf.fit(df=train_ds)\n",
        "\n",
        "    combined_df = rolling_evaluation(\n",
        "      model=nf,\n",
        "      full_df=dataset,\n",
        "      test_df=test_ds,\n",
        "      context_size=input_size,\n",
        "      horizon=h,\n",
        "      inteiro=False\n",
        "    )\n",
        "    clear_output(wait=False) # Limpa os textos da saida\n",
        "    # Extrair valores reais e previstos\n",
        "    actual = combined_df['y']\n",
        "    predicted = combined_df['LSTM']\n",
        "    # Calcular o RMSE (métrica a ser minimizada)\n",
        "    score = root_mean_squared_error(actual, predicted)\n",
        "\n",
        "    # Se o score atual for melhor que o melhor registrado, salva o modelo\n",
        "    if score < best_score_global:\n",
        "        best_score_global = score\n",
        "        combined_df.to_excel(f\"./Modelos/{local}/valores_predicao.xlsx\")\n",
        "        nf.save(path=f\"./Modelos/{local}/\",\n",
        "            model_index=None,\n",
        "            overwrite=True,\n",
        "            save_dataset=False\n",
        "        )\n",
        "        print(f\"\\nNovo melhor score: {score:.4f} - Modelo salvo!\\n\\n\")\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "0uZBJZCA3XCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Criação do estudo e otimização (n_trials pode ser ajustado conforme sua necessidade)\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Exibir os melhores resultados\n",
        "print(\"Melhor trial:\")\n",
        "trial = study.best_trial\n",
        "print(f\"  WMAPE: {trial.value}\")\n",
        "print(\"  Parâmetros:\")\n",
        "for key, value in trial.params.items():\n",
        "  print(f\"    {key}: {value}\")"
      ],
      "metadata": {
        "id": "LHhalzR40Idl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predições"
      ],
      "metadata": {
        "id": "ZR8DP-EncCX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O dataframe com as predições deve ter:\n",
        "* treino_id: um id que permite diferenciar o treino.\n",
        "* Unique_Id: O identificador de cada serie.\n",
        "* ds: a data de cada registro.\n",
        "* y: o valor real do registro para a data e a serie.\n",
        "* y_pred: o valor predito pelo modelo, caso não tenha deve preencher com NaN.\n",
        "* flag: indica se o registro foi usado no treino ou teste.\n",
        "* dataset: o nome do dataset usado para treinar e avaliar o modelo.\n",
        "* modelo: nome do algoritmo do modelo (LSTM, Randon Florest).\n",
        "* comentario: alguma anotação que pode ser util (pode ser vazio)\n",
        "* data do teino: data que o modelo foi treinado.\n"
      ],
      "metadata": {
        "id": "gQpHcDaecF1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# carregue o modelo\n",
        "path = \"/content/modelo\"\n",
        "print(f\"Diretório do modelo: {path}\")\n",
        "\n",
        "model = NeuralForecast.load(path=path)\n",
        "input_size = 4\n",
        "h = 1\n",
        "predictions = rolling_evaluation(\n",
        "    model=model,\n",
        "    full_df=dataset,\n",
        "    test_df=test_ds,\n",
        "    context_size=input_size,\n",
        "    horizon=h\n",
        ")"
      ],
      "metadata": {
        "id": "J8l9C8S6vRmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual = predictions['y']\n",
        "predicted = predictions['LSTM']\n",
        "# Calcular o WMAPE (métrica a ser minimizada)\n",
        "score = wmape(actual, predicted)\n",
        "score = score * 100\n",
        "\n",
        "# Calcular R² usando sklearn\n",
        "r2 = r2_score(actual, predicted)\n",
        "\n",
        "# Calcular RMSE usando sklearn\n",
        "rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
        "\n",
        "print(f\"WMAPE: {score:.2f}%\")\n",
        "print(f\"R²: {r2:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f} Toneladas\")"
      ],
      "metadata": {
        "id": "YMZ147xv-pq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Salva os resultados do treino."
      ],
      "metadata": {
        "id": "HfyysTeSJRPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colunas = [\n",
        "  'treino_id',    # Identificador do treino\n",
        "  'unique_id',    # Identificador de cada série\n",
        "  'ds',           # Data de cada registro\n",
        "  'y',            # Valor real\n",
        "  'y_pred',       # Valor predito pelo modelo\n",
        "  'diferença_%',  # Valor da diferença percentual entre o valor predito e o real\n",
        "  'flag',         # Indica se foi usado em treino ou teste\n",
        "  'dataset',      # Nome do dataset usado\n",
        "  'modelo',       # Nome do algoritmo (LSTM, Random Forest, etc)\n",
        "  'comentario',   # Anotações adicionais (pode ser vazio)\n",
        "  'data_treino'   # Data que o modelo foi treinado\n",
        "]\n",
        "\n",
        "# Verificar existência do arquivo\n",
        "arquivo_predicoes = 'predicoes_modelos.csv'\n",
        "\n",
        "if not os.path.exists(arquivo_predicoes):\n",
        "  # Criar DataFrame vazio com as colunas especificadas\n",
        "  df = pd.DataFrame(columns=colunas)\n",
        "\n",
        "  # Salvar o DataFrame vazio como CSV\n",
        "  df.to_csv(arquivo_predicoes, index=False)\n",
        "  print(f\"Arquivo {arquivo_predicoes} criado com DataFrame vazio.\")\n",
        "else:\n",
        "  df = pd.read_csv(arquivo_predicoes)\n",
        "  print(f\"Arquivo {arquivo_predicoes} já existe. Nenhuma ação necessária.\")"
      ],
      "metadata": {
        "id": "XGzq_yKgpEQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepara os dados do teste com os valore da predição."
      ],
      "metadata": {
        "id": "c8jtTG7vv2J7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dados_teste = predictions[['unique_id', 'ds', 'y', 'LSTM', 'diferença_%']].copy()\n",
        "dados_teste.columns = ['unique_id', 'ds', 'y', 'y_pred', 'diferença_%']\n",
        "dados_teste['flag'] = 'teste'"
      ],
      "metadata": {
        "id": "bJ5CiL9uqkYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepara os dados do treino, servirão como dados historicos para fazer os graficos posteriormente."
      ],
      "metadata": {
        "id": "pDpVIIVgwSWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dados_treino = dataset[['unique_id', 'ds', 'y']].copy()\n",
        "dados_treino['y'] = round(np.expm1(dados_treino['y']),2)\n",
        "\n",
        "inicio_teste = dados_teste['ds'].min() # identifica o periodo de inicio dos teste\n",
        "print(f\"Periodo de inicio dos testes: {inicio_teste}\\n\")\n",
        "dados_treino = dados_treino[dados_treino['ds'] < inicio_teste] # pega apenas as datas anteriores ao periodo de teste\n",
        "dados_treino['y_pred'] = \"-\"  # \"- \" para previsões no treino, nesse ponto elas não exitem.\n",
        "dados_treino['diferença_%'] = \"-\"  # \"-\" para a diferença percentual entre predito e real, nesse ponto elas não exitem.\n",
        "dados_treino['flag'] = 'treino'"
      ],
      "metadata": {
        "id": "iCzfiubYvp9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combina os dois dataframes em um unico."
      ],
      "metadata": {
        "id": "lsjze17cyIby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dados_completos = pd.concat([dados_treino, dados_teste], ignore_index=True)"
      ],
      "metadata": {
        "id": "-8k1unOVyDyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adiciona as informações sobre o treino atual."
      ],
      "metadata": {
        "id": "ETi5IBLMCsHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dados_completos['treino_id'] = \"terceira execução\""
      ],
      "metadata": {
        "id": "aAamYUMxDIF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dados_completos['comentario'] = \"primeira execução\""
      ],
      "metadata": {
        "id": "FhQX-bZwCoe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dados_completos['dataset'] = \"meu_dataset\"\n",
        "dados_completos['modelo'] = \"LSTM\"\n",
        "dados_completos['data_treino'] = datetime.now().strftime(\"%Y-%m-%d\")"
      ],
      "metadata": {
        "id": "gwIyCp6UCnuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Junta os dados do treino atual com os anteriores."
      ],
      "metadata": {
        "id": "HdWVo8iXDXR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nova_ordem_colunas = ['treino_id', 'unique_id', 'ds', 'y', 'y_pred', 'diferença_%', 'flag', 'dataset', 'modelo', 'comentario', 'data_treino']\n",
        "\n",
        "# Reordenar as colunas\n",
        "dados_completos = dados_completos[nova_ordem_colunas]"
      ],
      "metadata": {
        "id": "G1ZaC1aCJbfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = pd.concat([df, dados_completos], ignore_index=True)\n",
        "df_final.to_csv(arquivo_predicoes, index=False)"
      ],
      "metadata": {
        "id": "DfXMFTBODVfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Treinamentos realizados:\")\n",
        "for treino in df_final['treino_id'].unique():\n",
        "  print(f\" - {treino} ({df_final[df_final['treino_id']==treino]['data_treino'].unique()[0]})\")"
      ],
      "metadata": {
        "id": "wPDF_q5kP_Hn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}